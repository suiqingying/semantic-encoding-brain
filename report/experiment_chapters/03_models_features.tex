\section{模型选择、层抽样与特征提取流程}
本项目把刺激表征划分为三类：文本表征、音频表征与多模态表征。三类表征共享同一条评估主线：在 TR 对齐后形成 $(T, D)$ 的特征矩阵，经标准化与降维后引入时延展开，再用线性岭回归预测每个 ROI 的 fMRI。该框架的核心假设是：若某个表征捕获了与大脑语言加工相关的信息，则在控制对齐与动力学建模后，它应当在未见刺激上提供更高的预测相关。由于对齐性能对模型层级敏感，本文在模型内部以“多层取样”而非单层取值进行比较，这一做法与集成建模研究对层级结构的重要性强调一致 \cite{Schrimpf2021,Kumar2024}。

文本模型覆盖 \texttt{gpt2}、\texttt{bert-base-uncased} 与 \texttt{roberta-base}。三者代表了不同的上下文建模机制：BERT 以掩码语言建模学习双向上下文表示 \cite{Devlin2019}，RoBERTa 在更大规模训练与训练策略上对 BERT 做了系统改进，常被视为更强的双向基线；GPT-2 以自回归方式建模，隐藏状态更直接反映“左侧上下文对当前词的条件化” \cite{Radford2019}。在更早的上下文表征研究中，ELMo 通过双向语言模型得到深层上下文词表示 \cite{Peters2018}，其思想与本文“以预训练模型中间层作为可解释特征空间”的做法一致。本文实验实际运行的文本模型以 BERT/RoBERTa/GPT-2 为主，ELMo 作为相关工作参照用于解释上下文表征在神经对齐中的位置。

音频模型覆盖 \texttt{facebook/wav2vec2-base-960h}、\texttt{microsoft/wavlm-base-plus} 与 \texttt{facebook/hubert-base-ls960}。这些模型以波形为输入并产生帧级隐藏状态，其共同目标是学习可迁移的语音表征。wav2vec 2.0 的自监督目标通过遮蔽部分时间步并从上下文恢复其离散表示实现 \cite{Baevski2020}；相关脑对齐研究表明，自监督语音模型在层级上呈现从声学到更抽象结构的渐变，并在一定程度上与皮层语音加工层级对齐 \cite{Millet2023}。WavLM 与 HuBERT 在预训练目标与数据增强策略上与 wav2vec 家族有所差异，因而在“时间窗口依赖”与“最佳层位置”上可能呈现不同表现，这正是本文在统一评估协议下比较它们的动机之一。

多模态模型覆盖 \texttt{openai/whisper-small}、\texttt{openai/whisper-base} 与 \texttt{laion/clap-htsat-unfused}。Whisper 属于编码器—解码器结构，编码器把音频转换为序列表示，解码器在文本条件下生成输出；在本文的编码评估中，我们把其内部可获得的隐藏状态池化为 TR 级向量并进入同一回归框架。CLAP 属于双编码器结构，音频编码器与文本编码器被训练到共享嵌入空间；本文在固定 TR 窗口内构造音频片段与文本片段，以此得到多模态对齐表征。需要强调的是，多模态模型在本项目中的目标不是执行生成任务或检索任务，而是把其内部表征视为一种可比较特征空间；因此，我们只报告在当前实现中已经成功生成并保存 corr map 的配置，不对未完成或未保存的设置做推断。

层抽样策略采用“按相对深度等比例取样”。不同模型的层数并不相同，例如多数 base 级 Transformer 编码器为 12 层，但 Whisper-base 的可用层数更少，CLAP 的可用隐藏状态暴露方式也与标准 Transformer 不同。若直接固定绝对层号，会导致浅层模型越界或跨模型取样不均。本文在每个模型上先读取其总层数 $L$，再在 $[1, L]$ 上等间距取样得到若干层，并四舍五入去重，形成该模型的比较层集合。对 12 层模型，该策略通常得到 $\{1,4,6,9,12\}$；对 6 层模型得到 $\{1,2,4,5,6\}$。因此，本文中的“layer=$k$”应被理解为“该模型结构内的第 $k$ 层”，跨模型比较时把它视为从浅到深的相对位置，而不是跨模型共享的绝对语义层级。

特征提取在文本与音频侧分别包含两次聚合。文本侧首先构造 token 上下文窗口。本文固定上下文窗口为 200 token：对齐表给出每个词的顺序，代码将当前词之前最近的 200 个 token 作为窗口输入模型。随后在模型输出端进行 token 级池化以得到词窗口向量，本文在实际运行的配置中对自回归模型采用“最后 token 表征”，对双向模型同样使用窗口末端位置对应的表征作为稳定汇聚方式。第二次聚合发生在 TR 对齐阶段：同一 TR 内所有词向量取平均得到 TR 级文本特征。音频侧首先把音频按 TR 窗口切分为片段，模型输出的帧级隐藏状态在时间维上用 attention mask 做均值池化得到片段向量，从而与 TR 序列一一对应。多模态模型的输出同样被池化为 TR 级向量，并在后续步骤与单模态特征共享同一条处理与评估链路。
